{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/madushaninimeshika/mnist-nueralnetwork?scriptVersionId=215910583\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"## Imports\nimport torch\nimport torchvision ## Contains some utilities for working with the image data\nfrom torchvision.datasets import MNIST\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.295159Z","iopub.execute_input":"2025-01-03T08:50:18.295587Z","iopub.status.idle":"2025-01-03T08:50:18.301907Z","shell.execute_reply.started":"2025-01-03T08:50:18.295556Z","shell.execute_reply":"2025-01-03T08:50:18.300426Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"We will import torchvision which contains some utility functions for working with the image data. It also contain helper classes to automatically download and import the famous datasets like MNIST.\n\nMNIST dataset has 60,000 images which can be used to train the model. There is also an additional test set of 10,000 images which can be created by passing train = False to the MNIST class.\n\n# Loading the MNIST dataset","metadata":{}},{"cell_type":"code","source":"dataset = MNIST(root='data/', download=True)\nprint(len(dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.303263Z","iopub.execute_input":"2025-01-03T08:50:18.303687Z","iopub.status.idle":"2025-01-03T08:50:18.418524Z","shell.execute_reply.started":"2025-01-03T08:50:18.303615Z","shell.execute_reply":"2025-01-03T08:50:18.417501Z"}},"outputs":[{"name":"stdout","text":"60000\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.420399Z","iopub.execute_input":"2025-01-03T08:50:18.420751Z","iopub.status.idle":"2025-01-03T08:50:18.427889Z","shell.execute_reply.started":"2025-01-03T08:50:18.420722Z","shell.execute_reply":"2025-01-03T08:50:18.426828Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset MNIST\n    Number of datapoints: 60000\n    Root location: data/\n    Split: Train"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"image, label = dataset[59999]\nplt.imshow(image, cmap='gray')\nprint('Label : ', label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.429423Z","iopub.execute_input":"2025-01-03T08:50:18.429753Z","iopub.status.idle":"2025-01-03T08:50:18.633173Z","shell.execute_reply.started":"2025-01-03T08:50:18.429726Z","shell.execute_reply":"2025-01-03T08:50:18.632176Z"}},"outputs":[{"name":"stdout","text":"Label :  8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"These images are small in size, and recognizing the digits can sometimes be hard. PyTorch doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset.\n\nPyTorch datasets allow us to specify one or more transformation function which are applied to the images as they are loaded.\n\ntorchvision.transforms contains many such predefined functions and we will use ToTensor transform to convert images into Pytorch tensors.\nLoading the MNIST data with transformation applied while loadin","metadata":{}},{"cell_type":"code","source":"#MNIST Dataset(image and labels)\nmnist_dataset = MNIST(root='data/', train=True, transform=transforms.ToTensor())\nprint(mnist_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.634443Z","iopub.execute_input":"2025-01-03T08:50:18.634851Z","iopub.status.idle":"2025-01-03T08:50:18.735998Z","shell.execute_reply.started":"2025-01-03T08:50:18.634822Z","shell.execute_reply":"2025-01-03T08:50:18.734804Z"}},"outputs":[{"name":"stdout","text":"Dataset MNIST\n    Number of datapoints: 60000\n    Root location: data/\n    Split: Train\n    StandardTransform\nTransform: ToTensor()\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"image_tensor, label = mnist_dataset[0]\nprint(image_tensor.shape, label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.737072Z","iopub.execute_input":"2025-01-03T08:50:18.737378Z","iopub.status.idle":"2025-01-03T08:50:18.745108Z","shell.execute_reply.started":"2025-01-03T08:50:18.737352Z","shell.execute_reply":"2025-01-03T08:50:18.743944Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 28, 28]) 5\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"The image is now convert to a 28 X 28 tensor.The first dimension is used to keep track of the color channels. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in that case the color channels would be 3(Red, Green, Blue).\n","metadata":{}},{"cell_type":"code","source":"print(image_tensor[:, 10:15, 10: 15])\nprint(torch.max(image_tensor), torch.min(image_tensor))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.746235Z","iopub.execute_input":"2025-01-03T08:50:18.746538Z","iopub.status.idle":"2025-01-03T08:50:18.767909Z","shell.execute_reply.started":"2025-01-03T08:50:18.746511Z","shell.execute_reply":"2025-01-03T08:50:18.766566Z"}},"outputs":[{"name":"stdout","text":"tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\ntensor(1.) tensor(0.)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"The values range from 0 to 1, with 0 representing black, 1 white and the values between different shades of grey. We can also plot the tensor as an image using lt.imshow","metadata":{}},{"cell_type":"code","source":"# Plot the image of the tensor\nplt.imshow(image_tensor[0, 10:15, 10:15], cmap='gray')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.77059Z","iopub.execute_input":"2025-01-03T08:50:18.77095Z","iopub.status.idle":"2025-01-03T08:50:18.962732Z","shell.execute_reply.started":"2025-01-03T08:50:18.770917Z","shell.execute_reply":"2025-01-03T08:50:18.961225Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<matplotlib.image.AxesImage at 0x7a6d9856a920>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"The image is now convert to a 28 X 28 tensor.The first dimension is used to keep track of the color channels. Since images in the MNIST dataset are grayscale, there's just one channel. Other datasets have images with color, in that case the color channels would be 3(Red, Green, Blue).","metadata":{}},{"cell_type":"code","source":"train_data, validation_data = random_split(mnist_dataset, [50000, 10000])\n\n# Print the length of train & validation dataset\nprint(\"Length of Train dataset : \", len(train_data))\nprint(\"Length of validation dataset : \", len(validation_data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.964221Z","iopub.execute_input":"2025-01-03T08:50:18.964604Z","iopub.status.idle":"2025-01-03T08:50:18.975884Z","shell.execute_reply.started":"2025-01-03T08:50:18.964566Z","shell.execute_reply":"2025-01-03T08:50:18.974744Z"}},"outputs":[{"name":"stdout","text":"Length of Train dataset :  50000\nLength of validation dataset :  10000\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"These images are small in size, and recognizing the digits can sometimes be hard. PyTorch doesn't know how to work with images. We need to convert the images into tensors. We can do this by specifying a transform while creating our dataset.\n\nPyTorch datasets allow us to specify one or more transformation function which are applied to the images as they are loaded.\n\ntorchvision.transforms contains many such predefined functions and we will use ToTensor transform to convert images into Pytorch tensors.\n\nLoading the MNIST data with transformation applied while loading\n","metadata":{}},{"cell_type":"code","source":"x = torch.randn(4, 784)\nprint(x.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.977027Z","iopub.execute_input":"2025-01-03T08:50:18.977425Z","iopub.status.idle":"2025-01-03T08:50:18.98635Z","shell.execute_reply.started":"2025-01-03T08:50:18.977382Z","shell.execute_reply":"2025-01-03T08:50:18.985182Z"}},"outputs":[{"name":"stdout","text":"torch.Size([4, 784])\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Training loop","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass MnistModel(nn.Module):\n    def __init__(self, input_dim=28*28, output_dim=10):\n        super(MnistModel, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.network = nn.Sequential(\n            nn.Linear(self.input_dim, self.input_dim // 2),\n            nn.ReLU(),\n            nn.Linear(self.input_dim // 2, self.input_dim // 4),\n            nn.ReLU(),\n            nn.Linear(self.input_dim // 4, self.input_dim // 8),\n            nn.ReLU(),\n            nn.Linear(self.input_dim // 8, self.input_dim // 16),\n            nn.ReLU(),\n            nn.Linear(self.input_dim // 16, self.output_dim),\n            nn.Softmax(dim=-1)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\n# Hyperparameters\ninput_dim = 28 * 28\noutput_dim = 10\nbatch_size = 128\nepochs = 10\n\n# Initialize model, loss function, and optimizer\nmodel = MnistModel(input_dim, output_dim)\ncriterion = nn.CrossEntropyLoss()  # Suitable for multi-class classification\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\n\n# Move model to device (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Training loop\n\n# Data Loaders\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False)\n\nfor epoch in range(epochs):\n    model.train()  # Set model to train mode\n    running_loss = 0.0\n\n    for images, labels in train_loader:\n        # Flatten images into vectors of shape (batch_size, input_dim)\n        images = images.view(images.size(0), -1).to(device)  # Flatten input\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:50:18.987745Z","iopub.execute_input":"2025-01-03T08:50:18.988161Z","iopub.status.idle":"2025-01-03T08:51:43.363287Z","shell.execute_reply.started":"2025-01-03T08:50:18.988119Z","shell.execute_reply":"2025-01-03T08:51:43.362184Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 2.2989\nEpoch [2/10], Loss: 2.2273\nEpoch [3/10], Loss: 2.0800\nEpoch [4/10], Loss: 1.9548\nEpoch [5/10], Loss: 1.8853\nEpoch [6/10], Loss: 1.8172\nEpoch [7/10], Loss: 1.7755\nEpoch [8/10], Loss: 1.7449\nEpoch [9/10], Loss: 1.7223\nEpoch [10/10], Loss: 1.7058\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Evaluate the model\nUse the validation dataset toevaluate the model\n\nCompute the metrix like accuracy score","metadata":{}},{"cell_type":"code","source":"def ModelEvaluation(model, criterion, train_loader):\n    model.eval()\n    total_loss, total_correct = 0,0\n    with torch.no_grad():\n        for batch in train_loader:\n            images, labels = batch\n\n            # Flatten images\n            images = images.view(images.size(0), -1).to(device)\n            labels = labels.to(device)\n            \n            preds = model(images)\n            total_loss += criterion(preds, labels).item()\n            total_correct = (preds.argmax(1) == labels).sum().item()\n    return total_loss/len(train_loader), total_correct/len(train_loader.dataset)\n\nval_loss, val_acc = ModelEvaluation(model, criterion, val_loader)\nprint(f'Validation loss : {val_loss}, Accuracy : {val_acc}')\n                    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-03T08:57:00.467125Z","iopub.execute_input":"2025-01-03T08:57:00.467526Z","iopub.status.idle":"2025-01-03T08:57:01.815734Z","shell.execute_reply.started":"2025-01-03T08:57:00.467494Z","shell.execute_reply":"2025-01-03T08:57:01.814592Z"}},"outputs":[{"name":"stdout","text":"Validation loss : 1.7022625057003167, Accuracy : 0.0012\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}